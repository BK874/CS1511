\documentclass[letterpaper,notitlepage,twoside]{article}

% Basic imports, increase margins...
\usepackage[margin=0.75in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}

% Finite State Machine stuff
\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}

% Format tables nicely
\usepackage[latin1]{inputenc}
\usepackage{array}
\usepackage{booktabs}
\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}

\usepackage{amsfonts} 
\usepackage{amssymb}
\usepackage{amsmath,amsthm}
\usepackage{enumerate}

\renewcommand{\implies}{\Rightarrow} % redefine command "implies"  
\renewcommand{\iff}{\Leftrightarrow} % double arrow
\newcommand{\maps}{\rightarrow} % define command "map" 
\newcommand{\union}{\cup}
\newcommand{\intersect}{\cap}
\newcommand{\N}{\mathbb{N}} % natural number 
\newcommand{\Q}{\mathbb{Q}} % rational number 
\newcommand{\R}{\mathbb{R}} % real number 
\newcommand{\Z}{\mathbb{Z}} % integers 
\newcommand\tab[1][1cm]{\hspace*{#1}} %\tab command

% Add more packages that you use here...

\begin{document}
\title{Homework 7}
\author{Joe Baker, Brett Schreiber, Brian Knotten}
\maketitle

\section*{9}
The video details Leonard Susskind's argument for what happens to the information of objects that fall into black holes that ended the "Black Hole War" between he and Stephen Hawking. Leonard explains that, in string theory, elementary particles are viewed as having "vibrations on top of vibrations" that go faster the farther away they are from the particle. Because the gravity of a black hole warps time and space, objects appear (from an outside point of view) to slow down as they approach the event horizon, which allows an increasing amount of the vibrations to be seen. The vibrations appear as a 2D "scrambled mess" (referred to as a hologram) on the event horizon that represents the 3D object now at the center of the black hole. Therefore the information of an object that falls into a black hole is not lost: it is present both at the central mass and at the shimmering hologram at the event horizon. This holographic theory also applies to the entire universe as one can view reality both as the 3D world we perceive and as a flat, holographic film that exists at the edge of the universe.

\section*{10}

\subsection*{a}
\begin{enumerate}[(i)]

\item The entropy of  $X$ is:\\
$\begin{aligned}
H(X) &= P(x = 0) \cdot log(\frac{1}{P(x = 0)}) + P(x = 1) \cdot log(\frac{1}{P(x = 1)}) \\
&= \frac{1}{3} \cdot 1.58496 + \frac{2}{3} \cdot 0.58496 \\
&= 0.918293 \\ 
\end{aligned}$

\item The probability distribution $Y$ is: $P(y = 0) = \frac{11}{20}$ and $P(y = 1) = \frac{9}{20}$ \\

\item The entropy of $Y$ is: \\
$\begin{aligned}
H(Y) &= P(y = 0) \cdot log(\frac{1}{P(y = 0)}) + P(y = 1) \cdot log(\frac{1}{P(y = 1)}) \\
&= \frac{11}{20} \cdot 0.862496 + \frac{9}{20} \cdot 1.152003 \\
&= 0.992771 \\
\end{aligned}$

\item The conditional entropy $H(X | Y)$ is: \\
$\begin{aligned}
H(X|Y) &= P(x = 0 \cap y = 0) \cdot log(\frac{1}{P(0|0)}) + P(x = 0 \cap y = 1) \cdot log(\frac{1}{P(0|1)}) + P(x = 1 \cap y = 0) \cdot log(\frac{1}{P(1|0)}) \\ 
&+ P(x = 1 \cap y = 1) \cdot log(\frac{1}{P(1|1)}) \\
&= \frac{9}{30} \cdot 0.92891 + \frac{1}{30} \cdot 3.754908 + \frac{4}{30} \cdot 2.044396 + \frac{16}{30} \cdot 0.044394 \\
&= 0.700097 \\
\end{aligned}$

\item The conditional entropy $H(Y | X)$ is: \\
$\begin{aligned}
H(Y|X) &= P(x = 0 \cap y = 0) \cdot log(\frac{1}{P(0|0)}) + P(x = 0 \cap y = 1) \cdot log(\frac{1}{P(0|1)}) + P(x = 1 \cap y = 0) \cdot log(\frac{1}{P(1|0)}) \\ &+ P(x = 1 \cap y = 1) \cdot log(\frac{1}{P(1|1)}) \\
&= \frac{9}{30} \cdot 0.15203 + \frac{1}{30} \cdot 2.321928 + \frac{4}{30} \cdot 3.321928 + \frac{16}{30} \cdot 0.321928 \\
&= 0.737625 \\
\end{aligned}$

\item The mutual information $I(X ; Y)$ is: \\
$\begin{aligned}
I(X;Y) &= 0.918293 - 0.700097 \\ 
&= 0.218196 \\
\end{aligned}$

\item The mutual information $I(Y ; X)$ is: \\
$\begin{aligned}
I(Y;X) &= 0.992771 - 0.737625 \\
&= 0.255146 \\
\end{aligned}$

\item This fact means that knowing what the receiver outputted will reduce the entropy needed of the sender by the same amount that knowing the sender will reduce from the receiver. It means that the act of switching from one side to another is two-directional.

\end{enumerate}

\subsection*{b}
\begin{enumerate}[(i)]
\item Since mutual information measures the average reduction in uncertainty, then a negative reduction in uncertainty would mean an increase in uncertainty in some $x$ for knowing the value of some $y$. However, Shannon's Source Coding Theorem is sufficient. There is enough information in $H(X)$ to encode $x$. No more information is needed. So, in the worst case, knowing the value of $y$ won't provide any help in determining the value of $X$, that is, the lowest value of mutual information must be 0, since knowledge of the value of $y$ cannot increase the randomness of $x$ any further.
\item $I(X ; Y) = I(Y ; X)$ for all $X$ and $Y$ implies that mutual information always works both ways.
\end{enumerate}

\end{document}
